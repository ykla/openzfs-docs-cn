# 硬件

## 引言

在 ZFS 之前，存储依赖于相当昂贵的硬件，这些硬件无法防护静默损坏，而且扩展性也并不好。ZFS 的引入使人们能够使用比业内以往所用硬件便宜得多的设备，同时获得更优越的扩展能力。本页面试图为购买用于基于 ZFS 的服务器和工作站的硬件提供一些基本指导。

遵循这些指导的硬件将使 ZFS 在性能和可靠性方面发挥其全部潜力。不遵循这些指导的硬件则会成为掣肘。除非另有说明，这类掣肘适用于所有存储栈，并非 ZFS 所独有。使用竞争性存储栈构建的系统同样会从这些建议中受益。

## BIOS / CPU 微代码更新

强烈建议运行最新的 BIOS 和 CPU 微代码。

### 背景

计算机微处理器是非常复杂的设计，往往存在缺陷，这些缺陷被称为勘误（errata）。现代微处理器被设计为使用微代码。这将部分硬件设计置于一种准软件状态，使其无需更换整个芯片即可打补丁。勘误通常通过 CPU 微代码更新来解决。这些更新通常随 BIOS 更新一并提供。在某些情况下，BIOS 通过机器寄存器与 CPU 的交互也可以被修改，以在使用相同微代码的情况下修复问题。如果较新的微代码未作为 BIOS 更新的一部分提供，通常也可以由操作系统引导加载器或操作系统本身来加载。

## ECC 内存

比特翻转可能会对所有计算机文件系统造成相当严重的后果，ZFS 也不例外。ZFS（或任何其他文件系统）中使用的任何技术都无法防护比特翻转。因此，强烈推荐使用 ECC 内存。

### 背景

普通的背景辐射会随机翻转计算机内存中的比特，从而导致未定义的行为。这些被称为“比特翻转”。每一次比特翻转根据翻转的是哪一位，可能产生以下四种后果之一：

* 比特翻转可能没有任何影响。

  * 没有影响的比特翻转发生在未使用的内存中。
* 比特翻转可能导致运行时故障。

  * 当比特翻转发生在从磁盘读取的内容中时会出现这种情况。
  * 故障通常在程序代码被篡改时被观察到。
  * 如果比特翻转发生在系统内核或 /sbin/init 中的某个例程里，系统很可能会崩溃。否则，重新加载受影响的数据可以清除该问题，这通常通过重启来实现。
* 比特翻转可能导致数据损坏。

  * 当该比特正在被用于写入磁盘的数据时会出现这种情况。
  * 如果比特翻转发生在 ZFS 校验和计算之前，ZFS 将不会意识到数据已损坏。
  * 如果比特翻转发生在 ZFS 校验和计算之后、但在写出之前，ZFS 将检测到问题，但可能无法对其进行修复。
* 比特翻转可能导致元数据损坏。

  * 当比特翻转发生在正在写入磁盘的磁盘结构中时会出现这种情况。
  * 如果比特翻转发生在 ZFS 校验和计算之前，ZFS 将不会意识到元数据已损坏。
  * 如果比特翻转发生在 ZFS 校验和计算之后、但在写出之前，ZFS 将检测到问题，但可能无法对其进行修复。
  * 从这种事件中恢复将取决于被损坏的具体内容。在最坏的情况下，存储池可能会变得无法导入。

    * 所有文件系统在其绝对最坏情况下的比特翻转故障场景中可靠性都很差。这类场景应被视为极其罕见。

## 驱动器接口

### SAS 与 SATA

ZFS 依赖块设备层进行存储。因此，ZFS 会受到与其他文件系统相同因素的影响，例如驱动支持情况以及不可用的硬件。因此，有几点需要注意：

* 切勿在没有 SAS 转接器（interposer）的情况下，将 SATA 磁盘接入 SAS 扩展器。

  * 如果你这样做而且居然能工作，那只是例外，而不是常态。
* 不要指望 SAS 控制器能够兼容 SATA 端口倍增器。

  * 这种配置通常未经测试。
  * 磁盘可能无法被识别。
* 对 SATA 端口倍增器的支持在不同 OpenZFS 平台之间并不一致

  * Linux 驱动通常支持它们。
  * Illumos 驱动通常不支持它们。
  * FreeBSD 驱动在支持程度上介于 Linux 与 Illumos 之间。

### USB 硬盘和 / 或转接器

这些设备在扇区大小上报、SMART 透传、设置 ERC 的能力以及其他方面都存在问题。ZFS 在这类设备上的性能取决于它们本身所允许的能力，但应尽量避免使用它们。不应期望它们具有与 SAS 和 SATA 驱动器相同的运行时间，应将其视为不可靠。

## 控制器

理想的 ZFS 存储控制器应具备以下特性：

* 在主要 OpenZFS 平台上具备驱动支持

  * 稳定性很重要。
* 较高的单端口带宽

  * PCI Express 接口带宽除以端口数量
* 低成本

  * 不需要支持 RAID、电池备份单元以及硬件写缓存。

Marc Bevand 的博客文章 [From 32 to 2 ports: Ideal SATA/SAS Controllers for ZFS & Linux MD RAID](http://blog.zorinaq.com/?e=10) 列出了符合这些标准的存储控制器，是一份非常出色的清单。他会随着新控制器的出现而定期更新该列表。

### 硬件 RAID 控制器

不应将硬件 RAID 控制器与 ZFS 一起使用。尽管在硬件 RAID 之上运行时，ZFS 可能比其他文件系统更可靠，但其可靠性仍不如单独运行 ZFS 时高。

* 硬件 RAID 会限制 ZFS 在校验和失败时执行自我修复的能力。当 ZFS 执行 RAID-Z 或镜像时，某个磁盘上的校验和失败可以通过将包含该扇区的磁盘视为坏盘来重建原始数据进行修复。如果由 RAID 控制器处理冗余，则无法做到这一点，除非 ZFS 保存了重复副本。如果设置了 copies 标志，或者 RAID 属于 ZFS 内的镜像 / RAID-Z vdev，则元数据损坏可能可被修复。
* 硬件 RAID 在 RAID 1 上不一定能正确传递扇区大小信息。在 RAID 5/6 上无法正确传递扇区大小信息。硬件 RAID 1 更可能因部分扇区写入产生读-改-写开销，而硬件 RAID 5/6 几乎肯定会受到部分条带写入的影响（即 RAID 写洞）。ZFS 原生使用磁盘可以获取磁盘报告的扇区大小信息，从而避免扇区上的读-改-写，而 ZFS 通过写时复制（copy-on-write）设计避免了 RAID-Z 上的部分条带写入。

  * 当驱动器错误报告扇区大小时，ZFS 可能会出现扇区对齐问题。这类驱动器通常是基于 NAND 闪存的固态硬盘或来自高级格式（4K 扇区）过渡时期的旧 SATA 磁盘（Windows XP 停止支持前）。在创建 vdev 时可以[手动修正](https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Workload%20Tuning.html#alignment-shift-ashift)。
  * RAID 头可能导致 RAID 1 上的扇区写入错位，因为阵列可能从实际驱动器的某个扇区内开始，从而导致在创建 vdev 时手动修正扇区对齐无法解决问题。
* RAID 控制器故障可能需要使用同型号的控制器替换，或者在不那么极端的情况下使用同厂商的型号。单独使用 ZFS 可以使用任意控制器。
* 如果使用硬件 RAID 控制器的写缓存，会引入额外故障点，即便通过增加闪存以在断电事件中保存数据可以部分缓解。如果电池在断电事件中无法工作，或者没有闪存且电源未及时恢复，数据仍可能丢失。当缓存中有大量未完成写入时，写缓存中的数据丢失可能严重损坏 RAID 阵列上的内容。此外，所有写入都存储在缓存中，而不仅是需要写缓存的同步写入，这效率低下，且写缓存相对较小。ZFS 允许将同步写入直接写入闪存，这应能提供与硬件 RAID 类似的加速效果，并可加速更多正在进行的操作。
* 当静默损坏破坏数据时，RAID 重建期间的行为未定义。有报告称，当控制器遇到静默损坏时，RAID 5 和 6 阵列在重建期间可能丢失。ZFS 的校验和可以避免这种情况，通过判断是否有足够信息重建数据。如果没有，文件将在 zpool status 中被标记为损坏，系统管理员可以从备份中恢复。
* 每当操作系统阻塞在 IO 操作上时，由于系统 CPU 被用于 RAID 控制器的较弱嵌入式 CPU 阻塞，IO 响应时间会增加（即性能下降）。这会降低 IOPS，相较于 ZFS 可实现的性能。
* 控制器固件是额外的复杂层，任意第三方无法检查。ZFS 源代码是开源的，任何人都可以检查。
* 如果同一控制器形成多个 RAID 阵列，其中一个失败，操作系统看到的阵列标识可能会变得不一致。将磁盘直接交给操作系统可以通过映射到唯一端口或唯一磁盘标识来避免此问题。

  * 例如，如果你有阵列 A、B、C 和 D；阵列 B 失效，硬件 RAID 控制器与操作系统的交互可能将阵列 C 和 D 重命名为 B 和 C，从而导致从 cachefile 导入的池出现错误。
  * 并非所有 RAID 控制器都会这样。然而，该问题在 Linux 和 FreeBSD 上均有观察到，当系统管理员使用单盘 RAID 0 阵列时出现。此外，也在不同厂商的控制器上出现过。

有人可能倾向于尝试使用单盘 RAID 0 阵列，将 RAID 控制器当作 HBA 使用，但由于其他硬件 RAID 类型列出的多种原因，这并不推荐。为了性能和可靠性，最好使用 HBA 而非 RAID 控制器。

## 硬盘

### 扇区大小

历史上，所有硬盘的扇区大小为 512 字节，只有一些 SCSI 硬盘可以修改以支持稍大扇区。2009 年，业界从 512 字节扇区迁移到 4096 字节的“高级格式”（Advanced Format）扇区。由于 Windows XP 不兼容 4096 字节扇区或大于 2TB 的硬盘，一些早期高级格式硬盘实施了兼容 Windows XP 的变通方案。

* 市场上最早的高级格式硬盘为了兼容 Windows XP，将其扇区大小误报为 512 字节。截至 2013 年，这类硬盘被认为已不再生产。此时及之后生产的高级格式硬盘应报告其真实物理扇区大小。
* 存储 2TB 及以下的硬盘可能带有一个跳线，可设置将所有扇区偏移 1，以便为 Windows XP 提供正确对齐（Windows XP 的第一个分区从扇区 63 开始）。在 ZFS 上使用此类硬盘时，应关闭该跳线设置。

截至 2014 年，市场上仍有 512 字节和 4096 字节硬盘，但除非通过 USB 转 SATA 控制器，否则它们会正确识别自身。在使用 512 字节扇区硬盘创建的 vdev 中，用 4096 字节扇区硬盘替换会对性能产生负面影响。而用 512 字节扇区硬盘替换 4096 字节扇区硬盘则不会对性能造成影响。

### 错误恢复控制

ZFS 被认为可以使用廉价硬盘。这在 ZFS 刚推出且硬盘支持错误恢复控制时是正确的。自 ZFS 推出以来，某些厂商的低端硬盘（尤其是 Western Digital）已移除了错误恢复控制。要保证性能一致，需要支持错误恢复控制的硬盘。

#### 背景

硬盘通过磁性表面的微小极化区域存储数据。对该表面进行读取和/或写入会带来一些可靠性问题。一是表面缺陷可能导致比特损坏，二是振动可能导致磁头未命中目标。因此，硬盘扇区由三个区域组成：

* 扇区号
* 实际数据
* ECC

扇区号和 ECC 使硬盘能够检测并响应这些事件。当读取过程中发生上述任一事件时，硬盘会多次重试读取，直到成功或认定数据无法读取。后一种情况可能耗费大量时间，因此会导致对硬盘的 IO 阻塞。

企业级硬盘和部分消费级硬盘实现了以下功能：Western Digital 的 Time-Limited Error Recovery (TLER)、Seagate 的 Error Recovery Control (ERC) 以及 Hitachi 和 Samsung 的 Command Completion Time Limit，该功能允许系统管理员限制硬盘在此类事件上愿意花费的时间。

缺乏此功能的硬盘，其限制可能任意高。几分钟并非不可能。具有此功能的硬盘通常默认为 7 秒。ZFS 当前不会调整硬盘的此设置。不过，建议在 ZFS 被修改以控制它之前，编写脚本将错误恢复时间设置为较低值，例如 0.1 秒。此操作必须在每次启动时执行。

### 转速（RPM）

高转速硬盘具有更低的寻道时间，历来被认为更为理想。但它们增加了成本并牺牲存储密度，而实际提升通常不超过低转速硬盘的 6 倍。

举例来说，某主流厂商的 15k RPM 硬盘标称平均读取时间为 3.4 毫秒，平均写入时间为 3.9 毫秒。这个数值假设目标扇区距离磁头最多为盘片轨道数的一半和盘片半径的一半。若更远，则最坏情况下速度会减半。7200 RPM 硬盘的厂商标称值不可得，但经验测量平均为 13 到 16 毫秒。5400 RPM 硬盘预计更慢。

ARC 和 ZIL 能够缓解大部分因寻道时间降低而带来的性能优势。通过增加 ARC 的内存、L2ARC 设备和 SLOG 设备，可以获得更大幅度的 IOPS 提升。若将硬盘完全替换为固态存储，则性能提升更高。在考虑 IOPS 时，这类方案通常比高转速硬盘更具成本效益。

### 命令队列

支持命令队列的硬盘能够重新排序 IO 操作以提高 IOPS。SATA 上称为本地命令队列（Native Command Queuing），PATA/SCSI/SAS 上称为标记命令队列（Tagged Command Queuing）。ZFS 将对象存储在 metaslab 中，并可同时使用多个 metaslab。因此，ZFS 不仅设计为利用命令队列，而且良好的 ZFS 性能依赖命令队列。过去 10 年制造的几乎所有硬盘都支持命令队列。例外包括：

* 消费级 PATA/IDE 硬盘
* 2003 到 2004 年间使用 IDE 转 SATA 芯片的第一代 SATA 硬盘
* 在 BIOS 配置下运行 IDE 仿真的 SATA 硬盘

每个 OpenZFS 系统检查命令队列支持的方法不同。在 Linux 上使用 `hdparm -I /path/to/device | grep Queue`；在 FreeBSD 上使用 `camcontrol identify $DEVICE`。

## NAND 闪存 SSD

截至 2014 年，固态存储主要由 NAND 闪存主导，大多数关于固态存储的文章也几乎只关注 NAND 闪存。截至 2014 年，用于 ZFS 的最流行的闪存存储形式是带 SATA 接口的硬盘。带 SAS 接口的企业级型号开始逐渐面市。

截至 2017 年，市场上广泛提供使用 NAND 闪存且带 PCI-E 接口的固态存储。它们主要是企业级硬盘，使用 NVMe 接口，其开销低于 SATA 使用的 ATA 或 SAS 使用的 SCSI。还有一种称为 M.2 的接口，主要用于消费级 SSD，但不限于此。它可以提供多个总线的电气连接，如 SATA、PCI-E 和 USB。M.2 SSD 似乎使用 SATA 或 NVMe 接口。

### NVMe 低级格式化

许多 NVMe SSD 支持 512 字节扇区和 4096 字节扇区。它们通常出厂使用 512 字节扇区，而 512 字节扇区的性能低于 4096 字节扇区。有些还支持 T10/DIF CRC 元数据以提高可靠性，但对于 ZFS 来说这是不必要的。

在交给 ZFS 使用之前，应将 NVMe 硬盘[格式化](https://filers.blogspot.com/2018/12/how-to-format-nvme-drive.html)为使用 4096 字节扇区且不带元数据，以获得最佳性能，除非硬盘表明 512 字节扇区的性能与 4096 字节扇区相当，虽然这种情况不太可能。使用 `smartctl -a /dev/$device_namespace`（例如 `smartctl -a /dev/nvme1n1`）查看 Supported LBA Sizes 的 Rel_Perf 值，数值越低表示低级格式性能越高，0 为最佳。当前格式将在 Fmt 下标记加号。

可以使用以下命令格式化硬盘：

```
nvme format /dev/nvme1n1 -l $ID
```

其中 $ID 对应 Supported LBA Sizes SMART 信息中的 Id 字段值。

### 电源故障保护

#### 背景

闪存上的数据结构非常复杂，传统上极易受到损坏。在过去，此类损坏会导致 **整个** 硬盘数据丢失，且电源故障等事件可能导致多块硬盘同时损坏。由于硬盘固件无法审查，传统结论是，所有缺乏防止电源故障硬件特性的硬盘都不可靠，历史上多次验证了这一点 [1](https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Hardware.html#ssd-analysis) [2](https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Hardware.html#ssd-analysis2) [3](https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Hardware.html#ssd-analysis3)。自 2015 年起，关于电源故障导致 NAND 闪存 SSD 变砖的讨论几乎消失。SSD 厂商现在声称，固件电源丢失保护足够稳健，可以提供与硬件电源丢失保护等效的保护，[Kingston 是一个例子](https://www.kingston.com/us/solutions/servers-data-centers/ssd-power-loss-protection)。固件电源丢失保护用于保证已刷新数据和硬盘自身元数据的安全，这正是 ZFS 等文件系统所需的全部内容。

然而，那些需要或希望获得更强保障的人，如果希望固件漏洞不太可能在电源故障后使硬盘变砖，应继续使用提供硬件电源丢失保护的硬盘。关于硬件电源故障保护的基本工作原理，[Intel 已提供文档](https://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/ssd-power-loss-imminent-technology-brief.pdf)供有兴趣的人阅读细节。截至 2020 年，硬件电源丢失保护已成为企业级 SSD 的特性，这类 SSD 尝试在保护已刷新数据和硬盘元数据之外，还保护未刷新数据。对于 ZFS 来说，这种额外保护并无额外益处，但也不会造成影响。

还需注意，数据中心和笔记本电脑中的硬盘发生电源丢失事件的可能性较低，因此硬件电源丢失保护的实用性有限。尤其是在数据中心，冗余电源、UPS 电源以及使用 IPMI 强制重启，应该能防止大多数硬盘遇到电源丢失事件。

下文列出了提供硬件电源丢失保护的硬盘清单，供需要或希望使用的用户参考。由于 ZFS 与其他文件系统一样，仅要求对已刷新数据和硬盘元数据提供电源故障保护，因此清单中也包括只保护这些内容的旧硬盘。

#### 带电源故障保护的 NVMe 硬盘

以下是部分带电源故障保护的 NVMe 硬盘列表（非详尽）：

* Intel 750
* Intel DC P3500/P3600/P3608/P3700
* Kingston DC1000B（M.2 2280 尺寸，适用于大多数笔记本电脑）
* Micron 7300/7400/7450 PRO/MAX
* Samsung PM963（M.2 尺寸）
* Samsung PM1725/PM1725a
* Samsung XS1715
* Toshiba ZD6300
* Seagate Nytro 5000 M.2（XP1920LE30002 测试过；购买前请 **阅读下面备注**）

  * 这是一款价格低廉的 22110 M.2 企业级硬盘，使用面向消费者的 MLC，并针对以读取为主的工作负载进行了优化。它不适合作为 SLOG 设备（主要是写入负载）。
  * 该硬盘的 [手册](https://www.seagate.com/www-content/support-content/enterprise-storage/solid-state-drives/nytro-5000/_shared/docs/nytro-5000-mp2-pm-100810195d.pdf) 指定了气流要求。如果硬盘未能从机箱风扇获得足够气流，将在空闲状态下过热。其热节流将严重降低性能，使写入吞吐量降至规格的十分之一，并且读取延迟可达到数百毫秒。在持续负载下，设备温度会不断升高，直到出现“可靠性下降”事件，至少一个 NVMe 命名空间中的所有数据丢失。此时，必须进行安全擦除才能再次使用该 NVMe 命名空间。即使在正常情况下有足够气流，在企业环境下风扇故障时仍可能发生数据丢失。任何在企业环境中部署此硬盘的人都应注意这种故障模式。
  * 若希望在低气流环境中使用此硬盘，可以通过在 NAND 闪存控制器上安装被动散热片（例如 [这个](https://smile.amazon.com/gp/product/B07BDKN3XV)）来规避该故障模式。控制器位于贴纸下、最靠近电容的芯片上。本例测试是将散热片放在贴纸上（不拆贴纸）。散热片可以防止硬盘过热导致数据丢失，但在负载下没有主动气流的情况下，仍无法完全缓解过热问题。进行 scrub 操作时，读取几百 GB 数据后硬盘可能仍会过热。不过，热节流会迅速将温度从 76℃ 降至 74℃，恢复性能。

    * 在企业环境中可能通过散热片提供风扇故障后的数据保护，但这一点未进行评估。此外，为保证长期数据完整性，消费级 NAND 闪存的工作温度应保持在 40℃ 或以上。因此，在企业环境下使用散热片防止风扇故障导致数据丢失，应在投入生产前评估，以确保硬盘不会被过度冷却。

#### 带电源故障保护的 SAS 硬盘

以下是部分带电源故障保护的 SAS 硬盘列表（非详尽）：

* Samsung PM1633/PM1633a
* Samsung SM1625
* Samsung PM853T
* Toshiba PX05SHB***/PX04SHB***/PX04SHQ***
* Toshiba PX05SLB***/PX04SLB***/PX04SLQ***
* Toshiba PX05SMB***/PX04SMB***/PX04SMQ***
* Toshiba PX05SRB***/PX04SRB***/PX04SRQ***
* Toshiba PX05SVB***/PX04SVB***/PX04SVQ***

#### 带电源故障保护的 SATA 硬盘

以下是部分带电源故障保护的 SATA 硬盘列表（非详尽）：

* Crucial MX100/MX200/MX300
* Crucial M500/M550/M600
* Intel 320

  * 早期报告称 330 和 335 也带电源故障保护，[但实际上没有](https://engineering.nordeus.com/power-failure-testing-with-ssds)
* Intel 710
* Intel 730
* Intel DC S3500/S3510/S3610/S3700/S3710
* Kingston DC500R/DC500M
* Micron 5210 Ion

  * 列表中首款 QLC 硬盘，高容量且每 GB 价格低
* Samsung PM863/PM863a
* Samsung SM843T（不要与 SM843 混淆）
* Samsung SM863/SM863a
* Samsung 845DC Evo
* Samsung 845DC Pro

  * [高持续写入 IOPS](http://www.anandtech.com/show/8319/samsung-ssd-845dc-evopro-preview-exploring-worstcase-iops/5)
* Toshiba HK4E/HK3E2
* Toshiba HK4R/HK3R2/HK3R

#### 列表收录标准/流程

这些列表由 OpenZFS 贡献者（主要是 Richard Yao）基于可信信息源自愿整理。列表旨在保持厂商中立，不偏向任何特定制造商。若出现对某厂商的偏向，仅因缺乏相关信息或时间进行进一步调研。唯一收录要求是可靠来源确认硬盘具备充分的电源故障保护。充分的电源故障保护意味着硬盘必须保护自身内部元数据以及所有已刷新数据。未刷新数据的保护不相关，因此不是必要条件。ZFS 仅期望存储能够保护已刷新数据。因此，固态硬盘若其电源故障保护只覆盖已刷新数据，对 ZFS 保证数据安全已足够。

任何认为未列出的硬盘具备充分电源故障保护的人，可联系 [Mailing Lists](https://openzfs.github.io/openzfs-docs/Project%20and%20Community/Mailing%20Lists.html#mailing-lists) 请求收录，并提供该硬盘具备电源故障保护的证据。证据示例包括硬盘内部显示电容的图片、权威独立评测网站（如 Anandtech）的声明、以及厂商规格说明书。后者在未发现厂商夸大内部元数据保护或已刷新数据保护前，可视为诚信认可。到目前为止，所有厂商均诚信提供信息。

### Flash 页

NAND 芯片上可写入的最小单位是 flash 页。市场上最早的 NAND-flash SSD 页大小为 4096 字节。情况更复杂的是，此后页大小又翻倍了两次。NAND flash SSD **应**将这些页报告为扇区，但迄今所有 SSD 为兼容 Windows XP 均错误报告为 512 字节扇区。这与早期 advanced format 硬盘的情况类似。

截至 2014 年，大多数 NAND-flash SSD 页大小为 8192 字节。但部分厂商使用 128-Gbit NAND 的型号页大小为 16384 字节。最大性能要求 vdev 在创建时使用正确的 ashift 值（8192 字节对应 ashift=13，16384 字节对应 ashift=14）。然而，并非所有 OpenZFS 平台都支持该值。Linux 端口支持 ashift=13，而其他平台通常限制为 ashift=12（4096 字节）。

截至 2017 年，NAND-flash SSD 已针对 4096 字节 IO 优化。匹配 flash 页大小已不必要，通常 ashift=12 是正确选择。关于 flash 页大小的公开文档几乎不存在。

### ATA TRIM / SCSI UNMAP

需要注意，这与 zvol 上的 discard 或文件系统的 hole punching 是不同的情况。后者无论 ATA TRIM / SCSI UNMAP 是否发送到实际块设备都能正常工作。

#### ATA TRIM 性能问题

SATA 3.0 及之前版本中的 ATA TRIM 命令是非队列命令。在符合 SATA 3.0 或更早版本的 SATA 驱动器上发出 TRIM 命令时，驱动器会清空其 IO 队列并停止处理请求，直到完成 TRIM，这会影响性能。SATA 3.1 消除了这一限制，但市场上很少有 SATA 驱动器符合 SATA 3.1 标准，并且很难区分它们与 SATA 3.0 驱动器。同时，SCSI UNMAP 不存在此类问题。

## Optane / 3D XPoint SSD

这类 SSD 相比 NAND-flash SSD 拥有更低的延迟和更高的写入耐久性。它们支持按字节寻址，因此使用 ashift=9 即可。与 NAND-flash SSD 不同，它们不需要特殊的电源故障保护电路来保证可靠性，也无需运行 TRIM。不过，按 GB 价格比 NAND-flash 高（截至 2020 年）。企业型号非常适合作为 SLOG 设备。已知性能良好的型号包括：

* [Intel DC P4800X](https://www.servethehome.com/intel-optane-hands-on-real-world-benchmark-and-test-results/)
* [Intel DC P4801X](https://www.servethehome.com/intel-optane-dc-p4801x-review-100gb-m-2-nvme-ssd-log-option/)
* [Intel DC P1600X](https://www.servethehome.com/intel-optane-p1600x-small-capacity-ssd-for-boot-launched/)

注意，SLOG 设备在任何时刻通常使用的容量不超过 4GB，因此在成本方面，小容量设备通常是最佳选择。较大容量设备对 SLOG 无明显好处，但对于其他 vdev 类型，可能根据性能需求和成本考虑成为合适选择。

## 电源

强烈建议确保计算机正确接地。在用户家庭中曾有机器在接入没有接地线（即完全无地线）的插座时出现随机故障的案例。这种情况可能导致任何计算机系统随机故障，无论是否使用 ZFS。

电源也应相对稳定。应尽量避免电压大幅下降（棕色断电），可通过使用 UPS 或电源稳压器来缓解。受不稳定电源影响但未完全断电的系统可能会表现出未定义行为。具有较长保持时间的 PSU 可以提供部分保护，但保持时间通常未记录，也不能替代 UPS 或电源稳压器。

### PWR_OK 信号

PSU 应在电压不再符合额定规格时撤销 PWR_OK 信号，从而强制立即关机。然而，在一台开发工作站上观察到，在约 1 秒的棕色断电期间，系统时钟明显偏离预期值。当时该机器未使用 UPS。理论上 PWR_OK 机制应能防护此类情况，但实际观察表明 PWR_OK 机制不能作为严格保证。

### PSU 保持时间

PSU 保持时间是指在输入电源丢失后，PSU 在标准电压允许范围内继续以最大输出功率供电的时间。这对于支持 UPS 至关重要，因为标准 UPS 从电池供电所需的[转换时间](https://www.sunpower-uk.com/glossary/what-is-transfer-time/)可能让机器断电 5–12 毫秒。Intel 的《ATX 电源设计指南》规定最大连续输出下的保持时间为 17 毫秒。保持时间与 PSU 输出功率成反比，输出功率越低保持时间越长。

PSU 电容老化会降低保持时间，可能导致设备随时间可靠性下降。使用低于规格的 PSU 且保持时间不足的机器，需要高端 UPS 以确保转换时间不超过保持时间。如果在向电池供电转换期间保持时间低于转换时间，而 PWR_OK 信号未被撤销强制关机，可能会导致未定义行为。

如有疑问，应使用双转换 UPS。双转换 UPS 始终使用电池供电，使转换时间为 0。除非它们是高效混合型 UPS，虽然转换时间低于标准 PSU，但仍会增加延迟。也可以联系 PSU 厂商获取保持时间规格，但若需要多年可靠性，应使用高端 UPS 并确保低转换时间。

注意，除非支持高效模式，否则双转换 UPS 的效率最多为 94%，而高效模式会增加切换到电池供电的延迟。

### UPS 电池

UPS 单位中的铅酸电池通常需要定期更换，以确保在停电时能够供电。对于家庭系统，这通常是每 3 到 5 年一次，但具体周期会随温度变化而不同 [4](https://www.apc.com/us/en/faqs/FA158934/)。对于企业系统，请联系供应商。

脚注

[[1](http://lkcl.net/reports/ssd_analysis.html)] [SSD 电源故障分析报告](http://lkcl.net/reports/ssd_analysis.html)
[[2](https://www.usenix.org/system/files/conference/fast13/fast13-final80.pdf)] [USENIX FAST 2013 SSD 电源测试论文](https://www.usenix.org/system/files/conference/fast13/fast13-final80.pdf)
[[3](https://engineering.nordeus.com/power-failure-testing-with-ssds)] [Nordeus SSD 电源故障测试](https://engineering.nordeus.com/power-failure-testing-with-ssds)
[[4](https://www.apc.com/us/en/faqs/FA158934/)] [APC 关于 UPS 电池更换的常见问题](https://www.apc.com/us/en/faqs/FA158934/)
